{
    "subject": "Deep Learning",
    "practicals": [
        {
            "name": "XOR Using MLP",
            "files": ["main.py", "functions.py"],
            "aim": "Implement a Multi-Layer Perceptron (MLP) model to simulate the XOR Gate.",
            "con": "We successfully implemented a Multi-Layer Perceptron (MLP) model to solve the XOR problem, a classic example in machine learning that highlights the power of neural networks in capturing non-linear relationships."
        },
        {
            "name": "Gradient Descent",
            "files": ["main.py", "batch.py", "mini_batch.py", "SGD.py"],
            "aim": "Implement Batch GD, Mini-Batch GD and Stochastic GD algorithms to learn the parameters of supervised single layer Feedforward Neural Network.",
            "con": "Successfully implemented types of gradient descent."
        },
        {
            "name": "Backpropagation",
            "files": ["main.py"],
            "aim": "Implement Backpropagation algorithm to train a deep neural network with atleast 2 hidden layers.",
            "con": "We successfully implemented the backpropagation algorithm with two hidden layers."
        },
        {
            "name": "Autoencoder",
            "files": ["main.py"],
            "aim": "Design the architecture and implement the autoencoder model for image compression.",
            "con": "We successfully designed and implemented autoencoder model for image compression."
        }
    ]
}
